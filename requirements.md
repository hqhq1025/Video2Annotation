# Video to Annotation Tool Requirements (Revised)

This document outlines the requirements and initial thoughts for developing a video-to-annotation tool. The primary goal is to assist in creating annotated datasets for training machine learning models, with a specific focus on detecting scene changes within videos.

## Core Objective

- **Primary Function:** Detect scene changes in videos and record timestamps where these changes occur. Subsequently, use a vision model (QwenVL) to generate natural language annotations for these scenes.
- **Purpose:** Generate annotated data to train a machine learning model.

## Key Features & Workflow (Updated based on discussion)

1.  **Video Input:**
    *   Accept a single video file path as a command-line argument for the initial implementation (e.g., `python video2annotation.py /path/to/video.mp4`).
    *   A demo video `Bad.Boys.II.2003__#01-11-16_01-14-00_label_A_E0.mp4` is present in the project folder.
    *   Future enhancement could involve processing batches of videos.
2.  **Scene Change Detection:**
    *   Analyze the video to identify significant visual transitions indicating a new scene.
    *   The detection does not need to differentiate between hard cuts and soft transitions (fades, dissolves) initially; a general scene change detection is sufficient.
    *   Output precise start timestamps for each detected scene.
3.  **Post-Detection Processing (TBD):**
    *   **Option A: Video Segmentation:** Split the original video into separate clips, each corresponding to a scene.
    *   **Option B: Frame Extraction:** Extract key frames (e.g., the first frame of each new scene) as representatives of the scene.
    *   *Decision pending further clarification/input on preferred output for model training.*
4.  **Automated Annotation:**
    *   Utilize the QwenVL model to generate descriptive, natural language captions for each detected scene. This will be applied to either the segmented video clip or the extracted key frame representing the scene.
5.  **Output Format (TBD):**
    *   A structured dataset containing scene timestamps.
    *   Depending on the chosen processing path (segmentation or frame extraction), the output will include either video clips or extracted frames.
    *   Natural language annotations generated by QwenVL for each scene.
    *   *The exact final format for the training data file will be provided.*

## Tooling & Libraries (Preliminary)

- **Language:** Python (likely, due to common libraries for video processing and ML integration).
- **Video Processing:** Potentially OpenCV or FFmpeg-python for scene detection and manipulation.
- **Model Integration:** Integration with QwenVL for generating annotations. This might involve using a specific API or library provided for QwenVL.

## Next Steps

1.  ~~Clarify the choice between video segmentation and frame extraction.~~ (To be decided later based on training needs).
2.  ~~Define the specific requirements for the automated annotations.~~ (Defined: Natural language using QwenVL).
3.  ~~Decide on the initial technology stack.~~ (Likely Python, OpenCV/FFmpeg, QwenVL integration).
4.  ~~Determine the preferred user interface type.~~ (CLI - Command Line Interface).
5.  Provide the final desired output format for the training data file.
6.  ~~Identify the demo video in the project folder to understand the input.~~ (Identified: `Bad.Boys.II.2003__#01-11-16_01-14-00_label_A_E0.mp4`).